=== BEFORE TRAINING (empty Q-table) ===
step 00: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 01: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 02: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 03: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 04: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 05: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 06: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 07: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 08: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 09: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 10: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 11: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 12: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 13: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 14: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 15: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 16: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 17: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 18: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 19: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 20: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 21: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 22: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 23: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 24: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 25: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 26: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 27: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 28: state=(0, 0) action=   UP -> (0, 0) reward=-0.1
step 29: state=(0, 0) action=   UP -> (0, 0) reward=-0.1

=== TRAINING LOG (every 250 episodes) ===
episode=0250 eps=0.472
episode=0500 eps=0.223
episode=0750 eps=0.105
episode=1000 eps=0.050
episode=1250 eps=0.050
episode=1500 eps=0.050
episode=1750 eps=0.050
episode=2000 eps=0.050
episode=2250 eps=0.050
episode=2500 eps=0.050

=== AFTER TRAINING (learned greedy policy) ===
step 00: state=(0, 0) action= DOWN -> (1, 0) reward=-0.1
step 01: state=(1, 0) action=RIGHT -> (1, 1) reward=-0.1
step 02: state=(1, 1) action=RIGHT -> (1, 2) reward=-0.1
step 03: state=(1, 2) action=RIGHT -> (1, 3) reward=-0.1
step 04: state=(1, 3) action=RIGHT -> (1, 4) reward=-0.1
step 05: state=(1, 4) action= DOWN -> (2, 4) reward=-0.1
step 06: state=(2, 4) action= DOWN -> (3, 4) reward=-0.1
step 07: state=(3, 4) action= DOWN -> (4, 4) reward=+10.0
DONE: reached GOAL âœ…